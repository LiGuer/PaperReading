{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6991f9d3",
   "metadata": {},
   "source": [
    "# Citywide Cellular Traffic Prediction Based on Densely Connected Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da435a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D #空间三维画图\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7f06c",
   "metadata": {},
   "source": [
    "### Dataset Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f170759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76cd0998",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df62682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv1', nn.Conv2d(\n",
    "            num_input_features, bn_size * growth_rate,\n",
    "            kernel_size=1, stride=1, bias=False\n",
    "        ))\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv2', nn.Conv2d(\n",
    "            bn_size * growth_rate, growth_rate,\n",
    "            kernel_size=3, stride=1, padding=1, bias=False\n",
    "        ))\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, input):\n",
    "        new_features = super(_DenseLayer, self).forward(input)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(\n",
    "                new_features, p=self.drop_rate,\n",
    "                training=self.training\n",
    "            )\n",
    "        return torch.cat([input, new_features], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76e751df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate,bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae24c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetUnit(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, channels, nb_flows, layers=5, growth_rate=12,\n",
    "        num_init_features=32, bn_size=4, drop_rate=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if channels > 0:\n",
    "            self.features = nn.Sequential(OrderedDict([\n",
    "                ('conv0', nn.Conv2d(channels, num_init_features, kernel_size=3, padding=1)),\n",
    "                ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "                ('relu0', nn.ReLU(inplace=True))\n",
    "            ]))\n",
    "\n",
    "            # Dense Block\n",
    "            num_features = num_init_features\n",
    "            num_layers = layers\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers, num_input_features=num_features,\n",
    "                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate\n",
    "            )\n",
    "            self.features.add_module('denseblock', block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "\n",
    "            # Final batch norm\n",
    "            self.features.add_module('normlast', nn.BatchNorm2d(num_features))\n",
    "            self.features.add_module('convlast', nn.Conv2d(\n",
    "                num_features, nb_flows,\n",
    "                kernel_size=1, padding=0, bias=False\n",
    "            ))\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight.data)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    m.bias.data.zero_()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d881e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, nb_flows, drop_rate, channels):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.nb_flows = nb_flows\n",
    "        self.channels = channels\n",
    "\n",
    "        self.close_feature  = DenseNetUnit(channels[0], nb_flows)\n",
    "        self.period_feature = DenseNetUnit(channels[1], nb_flows)\n",
    "        self.trend_feature  = DenseNetUnit(channels[2], nb_flows)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = 0\n",
    "        if self.channels[0] > 0:\n",
    "            out += self.close_feature(inputs[0])\n",
    "        if self.channels[1] > 0:\n",
    "            out += self.period_feature(inputs[1])\n",
    "        if self.channels[2] > 0:\n",
    "            out += self.trend_feature(inputs[2])\n",
    "\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026e898",
   "metadata": {},
   "source": [
    "## Model Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ac9e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcbf7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optimizer, epoch, n_epochs, lr):\n",
    "    lr = lr\n",
    "    if float(epoch) / n_epochs > 0.75:\n",
    "        lr = lr * 0.01\n",
    "    if float(epoch) / n_epochs > 0.5:\n",
    "        lr = lr * 0.1\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09017b02",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0bbbe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_type='train', period_size, close_size, trend_size):\n",
    "    total_loss = 0\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        model.train()\n",
    "        data = train_loader\n",
    "        \n",
    "    elif data_type == 'valid':\n",
    "        model.eval()\n",
    "        data = valid_loader\n",
    "\n",
    "    if (period_size > 0) & (close_size > 0) & (trend_size > 0):\n",
    "        for idx, (c, p, t, target) in enumerate(data):\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            input_var = [Variable(_.float()).cuda(async=True) for _ in [c, p, t]]\n",
    "            target_var = Variable(target.float(), requires_grad=False).cuda(async=True)\n",
    "\n",
    "            pred = model(input_var)\n",
    "            loss = criterion(pred, target_var)\n",
    "            total_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    elif (close_size > 0) & (period_size > 0):\n",
    "        for idx, (c, p, target) in enumerate(data):\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            input_var = [Variable(_.float()).cuda(async=True) for _ in [c, p]]\n",
    "            target_var = Variable(target.float(), requires_grad=False).cuda(async=True)\n",
    "\n",
    "            pred = model(input_var)\n",
    "            loss = criterion(pred, target_var)\n",
    "            total_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    elif close_size > 0:\n",
    "        for idx, (c, target) in enumerate(data):\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            x = [Variable(c.float()).cuda(async=True)]\n",
    "            y = Variable(target.float(), requires_grad=False).cuda(async=True)\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(learningRate, optimizer, epochSize):\n",
    "    best_valid_loss = 1.0\n",
    "    train_loss, valid_loss = [], []\n",
    "    \n",
    "    for i in range(epochSize):\n",
    "        learningRate = set_lr(optimizer, i, epochSize, learningRate)\n",
    "        train_loss.append(train_epoch('train'))\n",
    "        valid_loss.append(train_epoch('valid'))\n",
    "\n",
    "        if valid_loss[-1] < best_valid_loss:\n",
    "            best_valid_loss = valid_loss[-1]\n",
    "\n",
    "            torch.save({'epoch': i, 'model': model, 'train_loss': train_loss,\n",
    "                        'valid_loss': valid_loss}, model_filename + '.model')\n",
    "            torch.save(optimizer, model_filename + '.optim')\n",
    "\n",
    "        print((\n",
    "            'iter: [{:d}/{:d}], train_loss: {:0.6f}, valid_loss: {:0.6f}, '\n",
    "            'best_valid_loss: {:0.6f}, lr: {:0.5f}'\n",
    "        ).format(\n",
    "            (i + 1), epochSize,\n",
    "            train_loss[-1], valid_loss[-1],\n",
    "            best_valid_loss, learningRate\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dbd6c",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6967920b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_15968/3933513678.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\29753\\AppData\\Local\\Temp/ipykernel_15968/3933513678.py\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    input_var = [Variable(_.float()).cuda(async=True) for _ in [c, p, t]]\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def predict(test_type='train'):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    loss = []\n",
    "    best_model = torch.load('../data/best.model').get('model')\n",
    "\n",
    "    if test_type == 'train':\n",
    "        data = train_loader\n",
    "    elif test_type == 'test':\n",
    "        data = test_loader\n",
    "    elif test_type == 'valid':\n",
    "        data = valid_loader\n",
    "\n",
    "    if (sizePeriod > 0) & (sizeClose > 0) & (sizeTrend > 0):\n",
    "        for idx, (c, p, t, target) in enumerate(data):\n",
    "            input_var = [Variable(_.float()).cuda(async=True) for _ in [c, p, t]]\n",
    "            target_var = Variable(target.float(), requires_grad=False).cuda(async=True)\n",
    "            pred = best_model(input_var)\n",
    "            predictions.append(pred.data.cpu().numpy())\n",
    "            ground_truth.append(target.numpy())\n",
    "            loss.append(criterion(pred, target_var).data[0])\n",
    "            \n",
    "    elif (sizeClose > 0) & (sizePeriod > 0):\n",
    "        for idx, (c, p, target) in enumerate(data):\n",
    "            input_var = [Variable(_.float()).cuda(async=True) for _ in [c, p]]\n",
    "            target_var = Variable(target.float(), requires_grad=False).cuda(async=True)\n",
    "            pred = best_model(input_var)\n",
    "            predictions.append(pred.data.cpu().numpy())\n",
    "            ground_truth.append(target.numpy())\n",
    "            loss.append(criterion(pred, target_var).data[0])\n",
    "            \n",
    "    elif sizeClose > 0:\n",
    "        for idx, (c, target) in enumerate(data):\n",
    "            input_var = Variable(c.float()).cuda(async=True)\n",
    "            target_var = Variable(target.float(), requires_grad=False).cuda(async=True)\n",
    "            pred = best_model(input_var)\n",
    "            predictions.append(pred.data.cpu().numpy())\n",
    "            ground_truth.append(target.numpy())\n",
    "            loss.append(criterion(pred, target_var).data[0])\n",
    "\n",
    "    final_predict = np.concatenate(predictions)\n",
    "    ground_truth = np.concatenate(ground_truth)\n",
    "\n",
    "    rmse = []\n",
    "    for y_hat, y in zip(final_predict, ground_truth):\n",
    "        flows, height, width = y_hat.shape\n",
    "        y_hat = np.reshape(y_hat, (flows, height * width)) * (mmn.max - mmn.min)\n",
    "        y = np.reshape(y, (flows, height * width)) * (mmn.max - mmn.min)\n",
    "        rmse.append(metrics.mean_squared_error(y_hat, y) ** 0.5)\n",
    "\n",
    "    if test_row & test_col:\n",
    "        row, col = test_row, test_col\n",
    "    else:\n",
    "        row_length, col_length = ground_truth.shape[-2:]\n",
    "        row, col = int(row_length/2), int(col_length/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890ea54",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc4ace",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32f39b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeClose  = 3\n",
    "sizePeriod = 0\n",
    "sizeTrend  = 0\n",
    "learningRate = 0.001\n",
    "batchSize = 32\n",
    "epochSize = 300\n",
    "dropRate = 0.0\n",
    "\n",
    "loss = 'l2'\n",
    "nb_flow = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dfab40d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'learningRate' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15968/607597648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15968/2861583167.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mlearningRate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'learningRate' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# get data channels\n",
    "channels = [sizeClose  * nb_flow,\n",
    "            sizePeriod * nb_flow,\n",
    "            sizeTrend  * nb_flow]\n",
    "\n",
    "model = DenseNet(\n",
    "    nb_flows = nb_flow, drop_rate = dropRate, channels = channels\n",
    ").cuda()\n",
    "optimizer = optim.Adam(model.parameters(), learningRate)\n",
    "\n",
    "if loss == 'l1':\n",
    "    criterion = nn.L1Loss().cuda()\n",
    "elif loss == 'l2':\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('test')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.load('../data/best.model').get('train_loss')[1:-1], 'r-')\n",
    "plt.legend(labels=['train_loss'], loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.load('../data/best.model').get('valid_loss')[:-1], 'k-')\n",
    "plt.legend(labels=['test_loss'], loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
